from datetime import timedelta

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.utils.dates import days_ago

from dynamic_tasks.custom_ops.trigger_multiple_dagruns import TriggerMultiDagRunOperator


docs = """
## Create dynamic tasks from triggering multiple DAGRUNs

This DAG shows how to run dynamic multiple tasks from result generated by previous tasks

using `TriggerMultiDagRunOperator` from [airflow_multi_dagrun](https://github.com/mastak/airflow_multi_dagrun)

### Before triggering DAG

There are 2 DAGs which is main task and sub task, main DAG creates dagruns for sub DAG while it does not wait for them to finished, make sure sub dags has its own methods to notify user if failed

### Pipeline

1. source: mimic dict results from multiple sources (dates are rendered and based on `execution_date`)
    * source1: `{"s1":["2021-09-05", "2021-09-04"]}`
    * source2: `{"s2":["2021-09-06"]}`
2. prepare_dynamic: read from `source1` and `source2`, merge the result to list and push to xcom
    * `[{'name': 's1', 'date': '2021-09-05'}, {'name': 's1', 'date': '2021-09-04'}, {'name': 's2', 'date': '2021-09-06'}]`
3. gen_target_dag_run: generate multiple target dagruns with dag_run config
4. dag `dynamic_single_task` generate 3 dagruns with config
    * `{'name': 's1', 'date': '2021-09-05'}`
    * `{'name': 's1', 'date': '2021-09-04'}`
    * `{'name': 's2', 'date': '2021-09-06'}`

"""

default_args = {
    'owner': 'sh',
    'depends_on_past': False,
    'retries': 0,
    'retry_delay': timedelta(minutes=10),
}

def _return_dynamic_source(**kwargs):
    return {kwargs["name"]: kwargs["dates"]}

def _merge_and_set_to_variable(*args, **kwargs):
    return_vals = kwargs["ti"].xcom_pull(key='return_value', task_ids=args)
    xcomval = list()
    for return_val in return_vals:
        for name in return_val:
            for d in return_val[name]:
                xcomval.append({"name": name, "date": d})
    return xcomval

def generate_dag_run(**context):
    """Callable can depend on the context"""
    task_confs = context["ti"].xcom_pull(key='return_value', task_ids='prepare_dynamic')
    for i, conf in enumerate(task_confs):
        yield {
            #'run_id': f"custom_trigger_id___{timezone.utcnow().isoformat()}",
            # whatever information you want to send to dag_run.conf
            'name': conf['name'],
            'date': conf['date'],
            'ds_from_context': context["ds"],
        }

with DAG(
    'dynamic_main_task',
    default_args=default_args,
    start_date=days_ago(1),
    description='demo how to generate dynamic task from variable',
    schedule_interval=None,
    catchup=False,
    tags=["sh", "dynamic"],
) as dag:

    dag.doc_md = docs

    s1 = PythonOperator(
        task_id="source1",
        python_callable=_return_dynamic_source,
        op_kwargs={
            "name": "s1",
            "dates": ['{{ ds }}', '{{ yesterday_ds }}'],
        },
    )

    s2 = PythonOperator(
        task_id="source2",
        python_callable=_return_dynamic_source,
        op_kwargs={
            "name": "s2",
            "dates": ['{{ tomorrow_ds }}'],
        },
    )

    prepare_task = PythonOperator(
        task_id='prepare_dynamic',
        python_callable=_merge_and_set_to_variable,
        op_args=[s1.task_id, s2.task_id],
    )

    gen_target_dag_run = TriggerMultiDagRunOperator(
        task_id='gen_target_dag_run',
        trigger_dag_id='dynamic_sub_task',
        python_callable=generate_dag_run,
    )

    end = DummyOperator(task_id='end')

    # DAG level dependencies
    [s1, s2] >> prepare_task >> gen_target_dag_run >> end
